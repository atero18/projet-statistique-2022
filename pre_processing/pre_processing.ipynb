{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy # pré-traitement\n",
    "from unidecode import unidecode # Suppression des accents\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réception des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/req_makeorg_environnement.json', encoding = \"utf8\") as f:\n",
    "    data = json.load(f)['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement du modèle linguistique (tokénisation, lemmes, stop words...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialisation_nlp() -> spacy.Language:\n",
    "    nlp = spacy.load(\"fr_core_news_md\")\n",
    "    stop_words_supp = {'>', '|', '<', '->', 'etc', 'etc.', '', 'ca', '=', 'e',}\n",
    "    nlp.Defaults.stop_words |= stop_words_supp\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = initialisation_nlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application du modèle et récupération des lemmes pour chaque proposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_traitement_documents(documents: str | list[str], nlp: spacy.Language = None) -> list[list[str]]:\n",
    "    if nlp is None:\n",
    "        nlp = initialisation_nlp()\n",
    "\n",
    "    if isinstance(documents, str):\n",
    "        documents = [documents]\n",
    "\n",
    "    docs = []\n",
    "\n",
    "    for doc in documents:\n",
    "        # Récupération de la phrase, suppression des accents\n",
    "        phrase = unidecode(doc['content'])\n",
    "\n",
    "        # passage en minuscules\n",
    "        phrase = phrase.lower()\n",
    "\n",
    "        # Remplacement (ou suppression) de certains caractères\n",
    "            # Chiffres\n",
    "        phrase = phrase.replace(\"co2\", \"co_deux\")\n",
    "        phrase = \"\".join(i for i in phrase if not i.isdigit())\n",
    "        phrase.replace(\"co_deux\", \"co2\")\n",
    "\n",
    "            # Symboles\n",
    "        phrase = phrase.replace('|', ' ')\n",
    "        phrase = phrase.replace('&', 'et')\n",
    "        phrase = phrase.replace('->', ' ')\n",
    "        phrase = phrase.replace('(s)','')\n",
    "        phrase = phrase.replace('.e.s', '')\n",
    "        phrase = phrase.replace('(', '')\n",
    "        phrase = phrase.replace(')', '')\n",
    "        phrase = phrase.replace('-', ' ')\n",
    "        phrase = phrase.replace('/', ' ')\n",
    "        phrase = phrase.replace('.', ' ')\n",
    "            # Erreurs\n",
    "        phrase = phrase.replace('toogoodtogo', '')\n",
    "        phrase = phrase.replace('passnavigo', 'pass navigo')\n",
    "            # Unions\n",
    "        unions = {\"mc do\": 'macdonald', \"fast food\": 'fastfood', \"costa rica\": 'costarica', \"union europeenne\": 'unioneuropeenne'}\n",
    "\n",
    "        for u in unions:\n",
    "            phrase = phrase.replace(u, unions[u])\n",
    "\n",
    "        # Expressions régulières\n",
    "        expreg: list[list[str]] = []\n",
    "        expreg.append([\"transport.{0,5}communs?\", \"transport_commun\"])\n",
    "        expreg.append([\"voyage.{0,5}communs?\", \"transport_commun\"])\n",
    "        expreg.append([\"politiques?\\spubliques?\", \"politique_publique\"])\n",
    "        expreg.append([\"huile.{0,5}palmes?\",\"huile_de_palm\"])\n",
    "            # Suppression des s en fin de mots\n",
    "        #expreg.append([\"s\\s+\", ' ']) \n",
    "        #expreg.append([\"s$\", ''])\n",
    "        for exp in expreg:\n",
    "            phrase = reg = re.compile(exp[0]).sub(exp[1], phrase)\n",
    "            \n",
    "\n",
    "            \n",
    "        # Tokenisation\n",
    "        phrase = nlp(phrase)\n",
    "\n",
    "        # Suppression de \"il faut\" en début de phrase\n",
    "        phrase = phrase[2:]\n",
    "\n",
    "        # Ajout de quelques conversions à faire au niveau du lemme\n",
    "        conversion = {'km': 'kilometre', 'cm': 'centimetre', 'in': 'dans', 'made': 'fabrique', 'ue': 'unioneuropeenne', 'wc': \"toilette\"}\n",
    "\n",
    "        docs.append([])\n",
    "        for token in phrase:\n",
    "            \n",
    "            lemme = token.lemma_.replace(' ', '')\n",
    "\n",
    "            # Suppression ponctuation et stop-word\n",
    "            # On ne garde que les lemmes de longueurs > 1 (enlève les lettres libres et les cases vides)\n",
    "            if not token.is_stop and not token.is_punct and len(lemme) > 1:\n",
    "                \n",
    "                if lemme in conversion:\n",
    "                    lemme = conversion[lemme]\n",
    "\n",
    "                docs[-1].append(lemme)\n",
    "\n",
    "    return docs[0] if len(docs) == 1 else docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pre_traitement_documents(data, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppression des redondances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(f\"Nombre de documents avant suppression des doublons : {len(docs)}\")\n",
    "\n",
    "    docsSansDoublons = []\n",
    "    [docsSansDoublons.append(doc) for doc in docs if doc not in docsSansDoublons]\n",
    "\n",
    "    print(f\"Nombre de documents après suppression des doublons : {len(docsSansDoublons)}\")\n",
    "    docs = docsSansDoublons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enregistrement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/docs.json\", 'w') as fichierJson:\n",
    "    json.dump(docs, fichierJson)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b30fa72fb85a99ca85a46e451030dfd87e2e54de7bf1c3c1af71cbda6f4de60"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
