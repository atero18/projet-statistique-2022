{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "from typing import Callable\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import itertools\n",
    "from comparaison_BATS import *\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from clustering import k_medoides as k_med\n",
    "import distance_wmd as wmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de documents : 9501\n"
     ]
    }
   ],
   "source": [
    "with open('../data/docs.json', encoding = \"utf8\") as f:\n",
    "    docs: list[str] = json.load(f)\n",
    "print(f\"Nombre de documents : {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire : 7231\n"
     ]
    }
   ],
   "source": [
    "with open('../data/liste_lemmes.txt') as f:\n",
    "    vocabulaire = f.readlines()\n",
    "\n",
    "# Suppression des retours chariots sur chaque ligne\n",
    "for i in range(len(vocabulaire)):\n",
    "    vocabulaire[i] = vocabulaire[i].replace('\\n', '')\n",
    "\n",
    "print(f\"Taille du vocabulaire : {len(vocabulaire)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création et entraînement basique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `vector_size` : taille de l'espace vectoriel de représentation des mots\n",
    "- `windows` : fenêtre gauche / droite de contexte\n",
    "- `min_count` : valeur fréquence absolue minimale que doit avoir un mot pour être inclus\n",
    "- `workers` : nombre de threads utilisés pour l'entraînement\n",
    "- `sg` : 1 si Skip-Gram, 0 si CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec: models.Word2Vec = models.Word2Vec(sentences = docs, vector_size = 300, window = 2, min_count = 1, workers = 6, sg = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# espace vectoriel généré des mots\n",
    "ev = w2vec.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enregistrement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec.save(\"../data/w2vec.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec: models.Word2Vec = models.Word2Vec.load(\"../data/w2vec.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# espace vectoriel généré des mots\n",
    "ev = w2vec.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation qualitative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximité de divers termes entre eux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mots_plus_proches(espace: models.KeyedVectors, mot: str, nbMots: int = 10, distance: str | Callable = 'cosine') -> list[str]:\n",
    "    resultats = {}\n",
    "    for v in vocabulaire:\n",
    "        if v != mot:\n",
    "            if distance == 'cosine':\n",
    "                resultats[v] = np.dot(espace[v], espace[mot]) / (np.linalg.norm(espace[v]) * np.linalg.norm(espace[mot]))\n",
    "\n",
    "    resultats = sorted(resultats.items(), key = lambda x: x[1], reverse = True)\n",
    "    for i in range(nbMots):\n",
    "        print(resultats[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('faire', 0.9997729)\n",
      "('nature', 0.9997668)\n",
      "('citoyen', 0.99976385)\n",
      "('être', 0.9997621)\n",
      "('france', 0.9997531)\n",
      "('environnement', 0.9997397)\n",
      "('biodiversite', 0.9997345)\n",
      "('pouvoir', 0.9997297)\n",
      "('pays', 0.99972695)\n",
      "('inciter', 0.9997259)\n"
     ]
    }
   ],
   "source": [
    "mots_plus_proches(ev, 'dechet', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mot_plus_proche(espace: models.KeyedVectors, vecteurMot: np.ndarray, nbMots: int = 5, distance: str | Callable = 'cosine') -> list[str]:\n",
    "    resultats = {}\n",
    "    for v in vocabulaire:\n",
    "        if distance == 'cosine':\n",
    "            resultats[v] = np.dot(espace[v], vecteurMot) / (np.linalg.norm(espace[v]) * np.linalg.norm(vecteurMot))\n",
    "\n",
    "    resultats = sorted(resultats.items(), key = lambda x: x[1], reverse = True)\n",
    "    for i in range(nbMots):\n",
    "        print(resultats[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dechet', 0.96101624)\n",
      "('selectif', 0.9606815)\n",
      "('chauffer', 0.96038085)\n",
      "('centre', 0.9601055)\n",
      "('eduquer', 0.9599925)\n"
     ]
    }
   ],
   "source": [
    "variation = ev['pluie'] - ev['eau']\n",
    "mot_plus_proche(ev, ev['dechet'] + variation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation quantitative (comparaison avec un modèle stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "traducteur = GoogleTranslator(source='en',target='fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "récupération d'un modèle word2vec de référence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vecR: models.KeyedVectors = models.KeyedVectors.load_word2vec_format(\"../data/frWiki_no_phrase_no_postag_1000_skip_cut100.bin\", binary=True, unicode_errors=\"ignore\")\n",
    "vocabulaireCommun = set(vocabulaire).intersection(w2vecR.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_proximite(modele: models.KeyedVectors, motCentral: str, mot2: str, listeMots: list[str] = None, distance: str = 'cosine') -> int:\n",
    "\n",
    "    if listeMots is None:\n",
    "        listeMots = modele.index_to_key\n",
    "\n",
    "    listeMots = listeMots.copy()\n",
    "    listeMots.remove(motCentral)\n",
    "    distMots: list[float] = []\n",
    "    pos = 1\n",
    "\n",
    "    if distance == 'cosine':\n",
    "        distMots = modele.distances(motCentral, listeMots)\n",
    "        temp = sorted(distMots)\n",
    "        pos = [temp.index(distMots[i]) for i in range(len(distMots)) if listeMots[i] == mot2][0] + 1\n",
    "        #pos = modele.rank(motCentral, mot2)\n",
    "    \n",
    "    return pos, pos / len(listeMots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3304, 0.857736240913811)\n",
      "(2284, 0.592938733125649)\n"
     ]
    }
   ],
   "source": [
    "print(get_position_proximite(w2vecR, \"vie\", \"chocolat\", list(vocabulaireCommun)))\n",
    "print(get_position_proximite(w2vec.wv,  \"vie\", \"chocolat\", list(vocabulaireCommun)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation des proximités sous BATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'taille_vocab': 3853,\n",
       " 'nb_comp': 179,\n",
       " 'rmse_freq': 0.38301971232118665,\n",
       " 'err_moy_freq': 0.23272048451671376,\n",
       " 'err_dis_cos': -0.6540805354466759,\n",
       " 'rmse_dis_cos': 0.6860216614593655}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stats_comparaisons_BATS(ev, w2vecR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moyenne des similarités entre chaque mot du vocabulaire des deux modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_comparaisons_intra_vocab(modele: models.KeyedVectors, reference: models.KeyedVectors):\n",
    "\n",
    "    vocabulaire = list(set(modele.index_to_key).intersection(reference.index_to_key))\n",
    "    stats = {}\n",
    "    stats['taille_vocab'] = len(vocabulaire)\n",
    "    stats['err_dis_cos'] = 0\n",
    "    n = 0\n",
    "    for mot1, mot2 in itertools.product(vocabulaire, vocabulaire):\n",
    "        n += 1\n",
    "        if mot1 == mot2:\n",
    "            continue\n",
    "        \n",
    "        # Distance cosinus\n",
    "        disMod = modele.distance(mot1, mot2)\n",
    "        disRef = modele.distance(mot1, mot2)\n",
    "        stats['err_dis_cos'] += disMod - disRef\n",
    "    \n",
    "    stats['err_dis_cos'] /= n\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_comparaisons_intra_vocab(ev, w2vecR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparamètres considérés : \n",
    "- Type de modèle (CBOW, Skip-Gram) ;\n",
    "- Taille de l'espace vectoriel (100 à 300) ;\n",
    "- Taille de la fenêtre de contexte (2 à 6) ;\n",
    "- Fréquence minimale des mots que l'on doit considérer (1, 2) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning(documents : list[str], modeleReference: models.KeyedVectors):\n",
    "\n",
    "    # Grille des hyperparamètres\n",
    "    typesModele = [0, 1]\n",
    "    taillesEV = range(100, 301, 50)\n",
    "    taillesFenetre = range(2, 7)\n",
    "    frequencesMin = [1, 2]\n",
    "\n",
    "    nbModeles: int = len(typesModele * len(taillesEV) * len(taillesFenetre) * len(frequencesMin))\n",
    "    print(f\"Nombre de modèles à tester : {nbModeles}\")\n",
    "    print(\"0 % | \", end = '')\n",
    "    tableauModeles = pd.DataFrame(columns = [\"type_modele\", \"dimension_ev\", \"taille_fenetre\", \"frequence_min\",\n",
    "    \"err_dis_cos\", 'rmse_dis_cos', 'err_moy_freq', 'rmse_freq'])\n",
    "\n",
    "    modelesFaits = 0\n",
    "    score = np.inf\n",
    "    bestModele: models.Word2Vec = None\n",
    "    meilleursHP = {}\n",
    "    # Réalisation d'un modèle pour chaque point de la grille\n",
    "    for typeModele, N, fenetre, frequenceMin in itertools.product(typesModele, taillesEV, taillesFenetre, frequencesMin):\n",
    "\n",
    "        modele = models.Word2Vec(sentences = documents,  vector_size = N, window = fenetre, min_count = frequenceMin, workers = 6, sg = typeModele)\n",
    "        stats = get_stats_comparaisons_BATS(modele.wv, modeleReference)\n",
    "\n",
    "        tableauModeles.loc[len(tableauModeles.index)] = [\"Skip-Gram\" if typeModele else \"CBOW\", N,\n",
    "        fenetre, frequenceMin, stats[\"err_dis_cos\"], stats[\"rmse_dis_cos\"], stats['err_moy_freq'], stats['rmse_freq']]\n",
    "        \n",
    "        modelesFaits += 1\n",
    "        if (percent := round(modelesFaits * 100 / nbModeles)) % 5 == 0 and round((modelesFaits -1) * 100 / nbModeles) != 0:\n",
    "            print(f\"{percent} % |\", end = \" \")\n",
    "\n",
    "    return tableauModeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    tableauModeles = tuning(docs, w2vecR)\n",
    "    tableauModeles\n",
    "    minRMSEDisCos = np.min(tableauModeles['rmse_dis_cos'])\n",
    "    tableauModeles.loc[tableauModeles['rmse_dis_cos'] == minRMSEDisCos, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Espace de documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Valeur moyenne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante transforme un document en vecteurs en faisant la moyenne de la représentation vectorielle de ses mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wvs_to_doc(doc: list[str], w2v: Callable, method: str = 'mean') -> np.ndarray:\n",
    "\n",
    "    if method == 'mean':\n",
    "\n",
    "        mean = np.zeros(len(w2v(doc[0])))\n",
    "        for mot in doc:\n",
    "            mean += w2v(mot)\n",
    "        mean /= len(doc)\n",
    "\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "toVec = lambda mot: ev[mot]\n",
    "\n",
    "meanDocs = []\n",
    "for doc in docs:\n",
    "    meanDocs.append(wvs_to_doc(doc, toVec))\n",
    "meanDocs = np.array(meanDocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Distance WMD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prend dans notre cas 3h45min environ de calcul\n",
    "if False:\n",
    "    distance_wmd_tous_docs(docs, ev, retour = 'fichier', nomFichier = 'wmd/w2vec.txt', toInteger = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = wmd.lecture_fichier_distances_wmd('../data/wmd/w2vec.txt', integer = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle 1 :\n",
      "Itération 1 : Création des groupes - Correction des tailles de groupe - part dans chaque groupe (%) : 5 5 80 5 5 (784 s) | Recherche du point moyen pour chaque groupe ( 0 1 2 3 4 ) - Stabilité des centres : X X X X X (194 s)\n",
      "Itération 2 : Création des groupes - Correction des tailles de groupe - part dans chaque groupe (%) : 6 10 39 5 39 (75 s) | Recherche du point moyen pour chaque groupe ( 0 1 2 3 4 ) - Stabilité des centres : V V X V V (97 s)\n",
      "Itération 3 : Création des groupes - Correction des tailles de groupe - part dans chaque groupe (%) : 6 9 61 5 19 (74 s) | Recherche du point moyen pour chaque groupe ( 0 1 2 3 4 ) - Stabilité des centres : V V V V V (128 s)\n",
      "Cycle 2 :\n",
      "Itération 1 : Création des groupes - Correction des tailles de groupe - part dans chaque groupe (%) : 80 5 5 5 5 (924 s) | Recherche du point moyen pour chaque groupe ( 0 1 2 3 4 ) - Stabilité des centres : X X X X X (205 s)\n",
      "Itération 2 : Création des groupes - Correction des tailles de groupe - part dans chaque groupe (%) : 5 41 24 23 7 (102 s) | Recherche du point moyen pour chaque groupe ( 0 1 2 3 4 ) - Stabilité des centres : X X X X X (93 s)\n",
      "Itération 3 : Création des groupes - Correction des tailles de groupe - part dans chaque groupe (%) : 59 5 11 16 8 (124 s) | Recherche du point moyen pour chaque groupe ( 0 1 2 3 4 ) - Stabilité des centres : X X X V X (129 s)\n",
      "Itération 4 : Création des groupes - part dans chaque groupe (%) : 10 58 10 14 7 (3 s) | Recherche du point moyen pour chaque groupe ( 0 1 2 3 4 ) - Stabilité des centres : V V X V V (122 s)\n",
      "Itération 5 : Création des groupes - part dans chaque groupe (%) : 10 59 8 15 8 (3 s) | Recherche du point moyen pour chaque groupe ( 0 1 2 3 4 ) - Stabilité des centres : V V X V V (125 s)\n",
      "Itération 6 : Création des groupes - part dans chaque groupe (%) : 10 59 7 15 9 (3 s) | Recherche du point moyen pour chaque groupe ( 0 1 2 3 4 ) - Stabilité des centres : V V V V V (127 s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       4\n",
       "4       4\n",
       "       ..\n",
       "9496    2\n",
       "9497    2\n",
       "9498    1\n",
       "9499    2\n",
       "9500    2\n",
       "Name: 2, Length: 9501, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupeDocs, positionCentres, nbIter, nbDistances, intraVar = k_med.k_medoides_wmd(docs, ev, distancesDocs = distances, graine = 1, nbCycles = 2)\n",
    "groupeDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbIter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : Sans donner une taille minimale on obtient quelque chose de type 97 0 3 0 0. Prend moins de temps à calculer par contre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération du vocabulaire et de la fréquence des mots dans chaque groupe de documents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerosGroupes = np.unique(groupeDocs)\n",
    "vocabGroupes = []\n",
    "for k in numerosGroupes:\n",
    "    vocabGroupes.append({})\n",
    "    for doc in [docs[i] for i in np.arange(len(docs))[groupeDocs == k]]:\n",
    "        mots, comptage = np.unique(doc, return_counts=True)\n",
    "        for i, mot in enumerate(mots):\n",
    "            if mot in vocabGroupes[-1]:\n",
    "                vocabGroupes[-1][mot] += comptage[i]\n",
    "            else:\n",
    "                vocabGroupes[-1][mot] = comptage[i]\n",
    "\n",
    "    vocabGroupes[-1] = {key: value for key, value in sorted(vocabGroupes[-1].items(), reverse = True, key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage du mot le plus utilisé dans chaque groupe :\n",
    "\n",
    "Ligne i : mot le plus utilisé dans le groupe i suivi du nombre de fois où on le retrouve dans le groupe 0, 1, ... etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transport : 442, 1, 18, 0, 11, \n",
      "\n",
      "plastique : 0, 357, 0, 305, 1, \n",
      "\n",
      "ville : 47, 2, 369, 1, 0, \n",
      "\n",
      "plastique : 0, 357, 0, 305, 1, \n",
      "\n",
      "produit : 10, 66, 9, 43, 689, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in vocabGroupes:\n",
    "    motPlusUtilise = list(d.keys())[0]\n",
    "    print(motPlusUtilise, end = ' : ')\n",
    "    for dPrim in vocabGroupes:\n",
    "        print(dPrim[motPlusUtilise] if motPlusUtilise in dPrim else '0', end = ', ')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ligne i : Mot le plus utilisé dans le groupe i et ayant le plus d'importance par rapport aux autres groupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transport [442, 1, 18, 0, 11]\n",
      "plastique [0, 357, 0, 305, 1]\n",
      "ville [47, 2, 369, 1, 0]\n",
      "bouteille [0, 29, 0, 222, 0]\n",
      "produit [10, 66, 9, 43, 689]\n"
     ]
    }
   ],
   "source": [
    "motsDisc: list[str] = [''] * len(vocabGroupes)\n",
    "quantiteMotsDisct: dict[list[int]] = {}\n",
    "for k, d in enumerate(vocabGroupes):\n",
    "    motPlusUtilise: str = None \n",
    "    freqMotPlusUtilise: float = 0\n",
    "    quantiteMotsDisct[k] = [0] * len(vocabGroupes)\n",
    "    \n",
    "\n",
    "    for mot in d:\n",
    "        temp: list[int] = []\n",
    "        for dPrim in vocabGroupes:\n",
    "            temp.append(dPrim[mot] if mot in dPrim else 0)\n",
    "        if d[mot] == max(temp) and (f := d[mot] / np.sum(temp) > freqMotPlusUtilise):\n",
    "            freqMotPlusUtilise = f\n",
    "            motPlusUtilise = mot\n",
    "            quantiteMotsDisct[k] = temp\n",
    "    \n",
    "    motsDisc[k] = motPlusUtilise\n",
    "    print(motPlusUtilise, quantiteMotsDisct[k])\n",
    "\n",
    "    \"\"\"\n",
    "    variationMotPlusUtilise: float = 0\n",
    "    for mot in d:\n",
    "        temp: list[int] = []\n",
    "        for dPrim in vocabGroupes:\n",
    "            temp.append(dPrim[mot] if mot in dPrim else 0)\n",
    "        temp = np.array(temp, dtype = np.float64)\n",
    "        if np.max(temp) == temp[k]:\n",
    "            n = np.sum(temp)\n",
    "            temp /= n\n",
    "            norme = np.linalg.norm(temp - np.max(temp), 2) ** 2\n",
    "            if norme > variationMotPlusUtilise:\n",
    "                variationMotPlusUtilise = norme\n",
    "                motPlusUtilise = mot\n",
    "    \n",
    "    motsDisc[k] = motPlusUtilise\n",
    "    print(k, temp, motPlusUtilise)\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b30fa72fb85a99ca85a46e451030dfd87e2e54de7bf1c3c1af71cbda6f4de60"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
